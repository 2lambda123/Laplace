<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>laplace.feature_extractor API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>laplace.feature_extractor</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import torch
import torch.nn as nn
from typing import Tuple, Callable, Optional


__all__ = [&#39;FeatureExtractor&#39;]


class FeatureExtractor(nn.Module):
    &#34;&#34;&#34;Feature extractor for a PyTorch neural network.
    A wrapper which returns the output of the penultimate layer in addition to
    the output of the last layer for each forward pass. It assumes that the
    last layer is linear.
    Based on https://gist.github.com/fkodom/27ed045c9051a39102e8bcf4ce31df76.

    Arguments
    ----------
    model : torch.nn.Module
        PyTorch model

    last_layer_name (optional) : str, default=None
        If the user already knows the name of the last layer, otherwise it will
        be determined automatically.

    Attributes
    ----------
    model : torch.nn.Module
        The underlying PyTorch model.

    last_layer : torch.nn.module
        The torch module corresponding to the last layer (has to be instance
        of torch.nn.Linear).

    Examples
    --------

    Notes
    -----
    Limitations:
        - Assumes that the last layer is always the same for any forward pass
        - Assumes that the last layer is an instance of torch.nn.Linear
    &#34;&#34;&#34;
    def __init__(self, model: nn.Module, last_layer_name: Optional[str] = None) -&gt; None:
        super().__init__()
        self.model = model
        self._features = dict()
        if last_layer_name is None:
            self._found = False
        else:
            self.set_last_layer(last_layer_name)

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        if self._found:
            # if last and penultimate layers are already known
            out = self.model(x)
        else:
            # if this is the first forward pass
            out = self.find_last_layer(x)
        return out

    def forward_with_features(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        out = self.forward(x)
        features = self._features[self._last_layer_name]
        return out, features

    def set_last_layer(self, last_layer_name: str) -&gt; None:
        # set last_layer attributes and check if it is linear
        self._last_layer_name = last_layer_name
        self.last_layer = dict(self.model.named_modules())[last_layer_name]
        if not isinstance(self.last_layer, nn.Linear):
            raise ValueError(&#39;Use model with a linear last layer.&#39;)

        # set forward hook to extract features in future forward passes
        self.last_layer.register_forward_hook(self._get_hook(last_layer_name))

        # last layer is now identified and hook is set
        self._found = True

    def _get_hook(self, name: str) -&gt; Callable:
        def hook(_, input, __):
            # only accepts one input (expects linear layer)
            self._features[name] = input[0].detach()
        return hook

    def find_last_layer(self, x: torch.Tensor) -&gt; torch.Tensor:
        if self._found:
            raise ValueError(&#39;Last layer is already known.&#39;)

        act_out = dict()
        def get_act_hook(name):
            def act_hook(_, input, __):
                # only accepts one input (expects linear layer)
                if isinstance(input[0], torch.Tensor):
                    act_out[name] = input[0].detach()
                else:
                    act_out[name] = None
                # remove hook
                handles[name].remove()
            return act_hook

        # set hooks for all modules
        handles = dict()
        for name, module in self.model.named_modules():
            handles[name] = module.register_forward_hook(get_act_hook(name))

        # check if model has more than one module
        # (there might be pathological exceptions)
        if len(handles) &lt;= 2:
            raise ValueError(&#39;The model only has one module.&#39;)

        # forward pass to find execution order
        out = self.model(x)

        # find the last layer, store features, return output of forward pass
        keys = list(act_out.keys())
        for key in reversed(keys):
            layer = dict(self.model.named_modules())[key]
            if len(list(layer.children())) == 0:
                self.set_last_layer(key)

                # save features from first forward pass
                self._features[key] = act_out[key]

                return out

        raise ValueError(&#39;Something went wrong (all modules have children).&#39;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="laplace.feature_extractor.FeatureExtractor"><code class="flex name class">
<span>class <span class="ident">FeatureExtractor</span></span>
<span>(</span><span>model: torch.nn.modules.module.Module, last_layer_name: Union[str, NoneType] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Feature extractor for a PyTorch neural network.
A wrapper which returns the output of the penultimate layer in addition to
the output of the last layer for each forward pass. It assumes that the
last layer is linear.
Based on <a href="https://gist.github.com/fkodom/27ed045c9051a39102e8bcf4ce31df76.">https://gist.github.com/fkodom/27ed045c9051a39102e8bcf4ce31df76.</a></p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>PyTorch model</dd>
</dl>
<p>last_layer_name (optional) : str, default=None
If the user already knows the name of the last layer, otherwise it will
be determined automatically.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>The underlying PyTorch model.</dd>
<dt><strong><code>last_layer</code></strong> :&ensp;<code>torch.nn.module</code></dt>
<dd>The torch module corresponding to the last layer (has to be instance
of torch.nn.Linear).</dd>
</dl>
<h2 id="examples">Examples</h2>
<h2 id="notes">Notes</h2>
<h2 id="limitations">Limitations</h2>
<ul>
<li>Assumes that the last layer is always the same for any forward pass</li>
<li>Assumes that the last layer is an instance of torch.nn.Linear</li>
</ul>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureExtractor(nn.Module):
    &#34;&#34;&#34;Feature extractor for a PyTorch neural network.
    A wrapper which returns the output of the penultimate layer in addition to
    the output of the last layer for each forward pass. It assumes that the
    last layer is linear.
    Based on https://gist.github.com/fkodom/27ed045c9051a39102e8bcf4ce31df76.

    Arguments
    ----------
    model : torch.nn.Module
        PyTorch model

    last_layer_name (optional) : str, default=None
        If the user already knows the name of the last layer, otherwise it will
        be determined automatically.

    Attributes
    ----------
    model : torch.nn.Module
        The underlying PyTorch model.

    last_layer : torch.nn.module
        The torch module corresponding to the last layer (has to be instance
        of torch.nn.Linear).

    Examples
    --------

    Notes
    -----
    Limitations:
        - Assumes that the last layer is always the same for any forward pass
        - Assumes that the last layer is an instance of torch.nn.Linear
    &#34;&#34;&#34;
    def __init__(self, model: nn.Module, last_layer_name: Optional[str] = None) -&gt; None:
        super().__init__()
        self.model = model
        self._features = dict()
        if last_layer_name is None:
            self._found = False
        else:
            self.set_last_layer(last_layer_name)

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        if self._found:
            # if last and penultimate layers are already known
            out = self.model(x)
        else:
            # if this is the first forward pass
            out = self.find_last_layer(x)
        return out

    def forward_with_features(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        out = self.forward(x)
        features = self._features[self._last_layer_name]
        return out, features

    def set_last_layer(self, last_layer_name: str) -&gt; None:
        # set last_layer attributes and check if it is linear
        self._last_layer_name = last_layer_name
        self.last_layer = dict(self.model.named_modules())[last_layer_name]
        if not isinstance(self.last_layer, nn.Linear):
            raise ValueError(&#39;Use model with a linear last layer.&#39;)

        # set forward hook to extract features in future forward passes
        self.last_layer.register_forward_hook(self._get_hook(last_layer_name))

        # last layer is now identified and hook is set
        self._found = True

    def _get_hook(self, name: str) -&gt; Callable:
        def hook(_, input, __):
            # only accepts one input (expects linear layer)
            self._features[name] = input[0].detach()
        return hook

    def find_last_layer(self, x: torch.Tensor) -&gt; torch.Tensor:
        if self._found:
            raise ValueError(&#39;Last layer is already known.&#39;)

        act_out = dict()
        def get_act_hook(name):
            def act_hook(_, input, __):
                # only accepts one input (expects linear layer)
                if isinstance(input[0], torch.Tensor):
                    act_out[name] = input[0].detach()
                else:
                    act_out[name] = None
                # remove hook
                handles[name].remove()
            return act_hook

        # set hooks for all modules
        handles = dict()
        for name, module in self.model.named_modules():
            handles[name] = module.register_forward_hook(get_act_hook(name))

        # check if model has more than one module
        # (there might be pathological exceptions)
        if len(handles) &lt;= 2:
            raise ValueError(&#39;The model only has one module.&#39;)

        # forward pass to find execution order
        out = self.model(x)

        # find the last layer, store features, return output of forward pass
        keys = list(act_out.keys())
        for key in reversed(keys):
            layer = dict(self.model.named_modules())[key]
            if len(list(layer.children())) == 0:
                self.set_last_layer(key)

                # save features from first forward pass
                self._features[key] = act_out[key]

                return out

        raise ValueError(&#39;Something went wrong (all modules have children).&#39;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="laplace.feature_extractor.FeatureExtractor.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="laplace.feature_extractor.FeatureExtractor.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="laplace.feature_extractor.FeatureExtractor.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
    if self._found:
        # if last and penultimate layers are already known
        out = self.model(x)
    else:
        # if this is the first forward pass
        out = self.find_last_layer(x)
    return out</code></pre>
</details>
</dd>
<dt id="laplace.feature_extractor.FeatureExtractor.forward_with_features"><code class="name flex">
<span>def <span class="ident">forward_with_features</span></span>(<span>self, x: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_with_features(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    out = self.forward(x)
    features = self._features[self._last_layer_name]
    return out, features</code></pre>
</details>
</dd>
<dt id="laplace.feature_extractor.FeatureExtractor.set_last_layer"><code class="name flex">
<span>def <span class="ident">set_last_layer</span></span>(<span>self, last_layer_name: str) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_last_layer(self, last_layer_name: str) -&gt; None:
    # set last_layer attributes and check if it is linear
    self._last_layer_name = last_layer_name
    self.last_layer = dict(self.model.named_modules())[last_layer_name]
    if not isinstance(self.last_layer, nn.Linear):
        raise ValueError(&#39;Use model with a linear last layer.&#39;)

    # set forward hook to extract features in future forward passes
    self.last_layer.register_forward_hook(self._get_hook(last_layer_name))

    # last layer is now identified and hook is set
    self._found = True</code></pre>
</details>
</dd>
<dt id="laplace.feature_extractor.FeatureExtractor.find_last_layer"><code class="name flex">
<span>def <span class="ident">find_last_layer</span></span>(<span>self, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_last_layer(self, x: torch.Tensor) -&gt; torch.Tensor:
    if self._found:
        raise ValueError(&#39;Last layer is already known.&#39;)

    act_out = dict()
    def get_act_hook(name):
        def act_hook(_, input, __):
            # only accepts one input (expects linear layer)
            if isinstance(input[0], torch.Tensor):
                act_out[name] = input[0].detach()
            else:
                act_out[name] = None
            # remove hook
            handles[name].remove()
        return act_hook

    # set hooks for all modules
    handles = dict()
    for name, module in self.model.named_modules():
        handles[name] = module.register_forward_hook(get_act_hook(name))

    # check if model has more than one module
    # (there might be pathological exceptions)
    if len(handles) &lt;= 2:
        raise ValueError(&#39;The model only has one module.&#39;)

    # forward pass to find execution order
    out = self.model(x)

    # find the last layer, store features, return output of forward pass
    keys = list(act_out.keys())
    for key in reversed(keys):
        layer = dict(self.model.named_modules())[key]
        if len(list(layer.children())) == 0:
            self.set_last_layer(key)

            # save features from first forward pass
            self._features[key] = act_out[key]

            return out

    raise ValueError(&#39;Something went wrong (all modules have children).&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="laplace" href="index.html">laplace</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="laplace.feature_extractor.FeatureExtractor" href="#laplace.feature_extractor.FeatureExtractor">FeatureExtractor</a></code></h4>
<ul class="">
<li><code><a title="laplace.feature_extractor.FeatureExtractor.forward" href="#laplace.feature_extractor.FeatureExtractor.forward">forward</a></code></li>
<li><code><a title="laplace.feature_extractor.FeatureExtractor.forward_with_features" href="#laplace.feature_extractor.FeatureExtractor.forward_with_features">forward_with_features</a></code></li>
<li><code><a title="laplace.feature_extractor.FeatureExtractor.set_last_layer" href="#laplace.feature_extractor.FeatureExtractor.set_last_layer">set_last_layer</a></code></li>
<li><code><a title="laplace.feature_extractor.FeatureExtractor.find_last_layer" href="#laplace.feature_extractor.FeatureExtractor.find_last_layer">find_last_layer</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>