<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>laplace.curvature API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>laplace.curvature</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import logging

from laplace.curvature.curvature import CurvatureInterface, GGNInterface, EFInterface

try:
    from laplace.curvature.backpack import BackPackGGN, BackPackEF, BackPackInterface
except ModuleNotFoundError:
    logging.info(&#39;Backpack not available.&#39;)

try:
    from laplace.curvature.asdf import AsdfGGN, AsdfEF, AsdfInterface
except ModuleNotFoundError:
    logging.info(&#39;asdfghjkl backend not available.&#39;)

__all__ = [&#39;CurvatureInterface&#39;, &#39;GGNInterface&#39;, &#39;EFInterface&#39;,
           &#39;BackPackInterface&#39;, &#39;BackPackGGN&#39;, &#39;BackPackEF&#39;,
           &#39;AsdfInterface&#39;, &#39;AsdfGGN&#39;, &#39;AsdfEF&#39;]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="laplace.curvature.asdf" href="asdf.html">laplace.curvature.asdf</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="laplace.curvature.backpack" href="backpack.html">laplace.curvature.backpack</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="laplace.curvature.curvature" href="curvature.html">laplace.curvature.curvature</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="laplace.curvature.CurvatureInterface"><code class="flex name class">
<span>class <span class="ident">CurvatureInterface</span></span>
<span>(</span><span>model, likelihood, last_layer=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Interface to access curvature for a model and corresponding likelihood.
A <code><a title="laplace.curvature.CurvatureInterface" href="#laplace.curvature.CurvatureInterface">CurvatureInterface</a></code> must inherit from this baseclass and implement the
necessary functions <code>jacobians</code>, <code>full</code>, <code>kron</code>, and <code>diag</code>.
The interface might be extended in the future to account for other curvature
structures, for example, a block-diagonal one.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code> or <code><a title="laplace.feature_extractor.FeatureExtractor" href="../feature_extractor.html#laplace.feature_extractor.FeatureExtractor">FeatureExtractor</a></code></dt>
<dd>torch model (neural network)</dd>
<dt><strong><code>likelihood</code></strong> :&ensp;<code>{'classification', 'regression'}</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>last_layer</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>only consider curvature of last layer</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>lossfunc</code></strong> :&ensp;<code>torch.nn.MSELoss</code> or <code>torch.nn.CrossEntropyLoss</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>factor</code></strong> :&ensp;<code>float</code></dt>
<dd>conversion factor between torch losses and base likelihoods
For example, <span><span class="MathJax_Preview">\frac{1}{2}</span><script type="math/tex">\frac{1}{2}</script></span> to get to <span><span class="MathJax_Preview">\mathcal{N}(f, 1)</span><script type="math/tex">\mathcal{N}(f, 1)</script></span> from MSELoss.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CurvatureInterface(ABC):
    &#34;&#34;&#34;Interface to access curvature for a model and corresponding likelihood.
    A `CurvatureInterface` must inherit from this baseclass and implement the
    necessary functions `jacobians`, `full`, `kron`, and `diag`.
    The interface might be extended in the future to account for other curvature
    structures, for example, a block-diagonal one.

    Parameters
    ----------
    model : torch.nn.Module or `laplace.feature_extractor.FeatureExtractor`
        torch model (neural network)
    likelihood : {&#39;classification&#39;, &#39;regression&#39;}
    last_layer : bool, default=False
        only consider curvature of last layer

    Attributes
    ----------
    lossfunc : torch.nn.MSELoss or torch.nn.CrossEntropyLoss
    factor : float
        conversion factor between torch losses and base likelihoods
        For example, \\(\\frac{1}{2}\\) to get to \\(\\mathcal{N}(f, 1)\\) from MSELoss.
    &#34;&#34;&#34;
    def __init__(self, model, likelihood, last_layer=False):
        assert likelihood in [&#39;regression&#39;, &#39;classification&#39;]
        self.likelihood = likelihood
        self.model = model
        self.last_layer = last_layer
        if likelihood == &#39;regression&#39;:
            self.lossfunc = MSELoss(reduction=&#39;sum&#39;)
            self.factor = 0.5
        else:
            self.lossfunc = CrossEntropyLoss(reduction=&#39;sum&#39;)
            self.factor = 1.

    @property
    def _model(self):
        return self.model.last_layer if self.last_layer else self.model

    @abstractstaticmethod
    def jacobians(model, x):
        &#34;&#34;&#34;Compute Jacobians \\(\\nabla_\\theta f(x;\\theta)\\) at current parameter \\(\\theta\\).

        Parameters
        ----------
        model : torch.nn.Module
        x : torch.Tensor
            input data `(batch, input_shape)` on compatible device with model.

        Returns
        -------
        Js : torch.Tensor
            Jacobians `(batch, parameters, outputs)`
        f : torch.Tensor
            output function `(batch, outputs)`
        &#34;&#34;&#34;
        pass

    @staticmethod
    def last_layer_jacobians(model, x):
        &#34;&#34;&#34;Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) 
        only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).

        Parameters
        ----------
        model : laplace.feature_extractor.FeatureExtractor
        x : torch.Tensor

        Returns
        -------
        Js : torch.Tensor
            Jacobians `(batch, last-layer-parameters, outputs)`
        f : torch.Tensor
            output function `(batch, outputs)`
        &#34;&#34;&#34;
        f, phi = model.forward_with_features(x)
        bsize = len(x)
        output_size = f.shape[-1]

        # calculate Jacobians using the feature vector &#39;phi&#39;
        identity = torch.eye(output_size, device=x.device).unsqueeze(0).tile(bsize, 1, 1)
        # Jacobians are batch x output x params
        Js = torch.einsum(&#39;kp,kij-&gt;kijp&#39;, phi, identity).reshape(bsize, output_size, -1)
        if model.last_layer.bias is not None:
            Js = torch.cat([Js, identity], dim=2)

        return Js, f.detach()

    @abstractstaticmethod
    def gradients(self, x, y):
        &#34;&#34;&#34;Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\).

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)` on compatible device with model.
        y : torch.Tensor

        Returns
        -------
        loss : torch.Tensor
        Gs : torch.Tensor
            gradients `(batch, parameters)`
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def full(self, x, y, **kwargs):
        &#34;&#34;&#34;Compute a dense curvature (approximation) in the form of a \\(P \\times P\\) matrix
        \\(H\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\).

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)`
        y : torch.Tensor
            labels `(batch, label_shape)`

        Returns
        -------
        loss : torch.Tensor
        H : torch.Tensor
            Hessian approximation `(parameters, parameters)`
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def kron(self, x, y, **kwargs):
        &#34;&#34;&#34;Compute a Kronecker factored curvature approximation (such as KFAC).
        The approximation to \\(H\\) takes the form of two Kronecker factors \\(Q, H\\),
        i.e., \\(H \\approx Q \\otimes H\\) for each Module in the neural network permitting 
        such curvature.
        \\(Q\\) is quadratic in the input-dimension of a module \\(p_{in} \\times p_{in}\\)
        and \\(H\\) in the output-dimension \\(p_{out} \\times p_{out}\\).

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)`
        y : torch.Tensor
            labels `(batch, label_shape)`

        Returns
        -------
        loss : torch.Tensor
        H : `laplace.matrix.Kron`
            Kronecker factored Hessian approximation.
        &#34;&#34;&#34;

    @abstractmethod
    def diag(self, x, y, **kwargs):
        &#34;&#34;&#34;Compute a diagonal Hessian approximation to \\(H\\) and is represented as a 
        vector of the dimensionality of parameters \\(\\theta\\).

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)`
        y : torch.Tensor
            labels `(batch, label_shape)`

        Returns
        -------
        loss : torch.Tensor
        H : torch.Tensor
            vector representing the diagonal of H
        &#34;&#34;&#34;
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="laplace.curvature.asdf.AsdfInterface" href="asdf.html#laplace.curvature.asdf.AsdfInterface">AsdfInterface</a></li>
<li><a title="laplace.curvature.backpack.BackPackInterface" href="backpack.html#laplace.curvature.backpack.BackPackInterface">BackPackInterface</a></li>
<li><a title="laplace.curvature.curvature.EFInterface" href="curvature.html#laplace.curvature.curvature.EFInterface">EFInterface</a></li>
<li><a title="laplace.curvature.curvature.GGNInterface" href="curvature.html#laplace.curvature.curvature.GGNInterface">GGNInterface</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="laplace.curvature.CurvatureInterface.jacobians"><code class="name flex">
<span>def <span class="ident">jacobians</span></span>(<span>model, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute Jacobians <span><span class="MathJax_Preview">\nabla_\theta f(x;\theta)</span><script type="math/tex">\nabla_\theta f(x;\theta)</script></span> at current parameter <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>input data <code>(batch, input_shape)</code> on compatible device with model.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Js</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Jacobians <code>(batch, parameters, outputs)</code></dd>
<dt><strong><code>f</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>output function <code>(batch, outputs)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractstaticmethod
def jacobians(model, x):
    &#34;&#34;&#34;Compute Jacobians \\(\\nabla_\\theta f(x;\\theta)\\) at current parameter \\(\\theta\\).

    Parameters
    ----------
    model : torch.nn.Module
    x : torch.Tensor
        input data `(batch, input_shape)` on compatible device with model.

    Returns
    -------
    Js : torch.Tensor
        Jacobians `(batch, parameters, outputs)`
    f : torch.Tensor
        output function `(batch, outputs)`
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="laplace.curvature.CurvatureInterface.last_layer_jacobians"><code class="name flex">
<span>def <span class="ident">last_layer_jacobians</span></span>(<span>model, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute Jacobians <span><span class="MathJax_Preview">\nabla_{\theta_\textrm{last}} f(x;\theta_\textrm{last})</span><script type="math/tex">\nabla_{\theta_\textrm{last}} f(x;\theta_\textrm{last})</script></span>
only at current last-layer parameter <span><span class="MathJax_Preview">\theta_{\textrm{last}}</span><script type="math/tex">\theta_{\textrm{last}}</script></span>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code><a title="laplace.feature_extractor.FeatureExtractor" href="../feature_extractor.html#laplace.feature_extractor.FeatureExtractor">FeatureExtractor</a></code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Js</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Jacobians <code>(batch, last-layer-parameters, outputs)</code></dd>
<dt><strong><code>f</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>output function <code>(batch, outputs)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def last_layer_jacobians(model, x):
    &#34;&#34;&#34;Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) 
    only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).

    Parameters
    ----------
    model : laplace.feature_extractor.FeatureExtractor
    x : torch.Tensor

    Returns
    -------
    Js : torch.Tensor
        Jacobians `(batch, last-layer-parameters, outputs)`
    f : torch.Tensor
        output function `(batch, outputs)`
    &#34;&#34;&#34;
    f, phi = model.forward_with_features(x)
    bsize = len(x)
    output_size = f.shape[-1]

    # calculate Jacobians using the feature vector &#39;phi&#39;
    identity = torch.eye(output_size, device=x.device).unsqueeze(0).tile(bsize, 1, 1)
    # Jacobians are batch x output x params
    Js = torch.einsum(&#39;kp,kij-&gt;kijp&#39;, phi, identity).reshape(bsize, output_size, -1)
    if model.last_layer.bias is not None:
        Js = torch.cat([Js, identity], dim=2)

    return Js, f.detach()</code></pre>
</details>
</dd>
<dt id="laplace.curvature.CurvatureInterface.gradients"><code class="name flex">
<span>def <span class="ident">gradients</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute gradients <span><span class="MathJax_Preview">\nabla_\theta \ell(f(x;\theta, y)</span><script type="math/tex">\nabla_\theta \ell(f(x;\theta, y)</script></span> at current parameter <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>input data <code>(batch, input_shape)</code> on compatible device with model.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>loss</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>Gs</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>gradients <code>(batch, parameters)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractstaticmethod
def gradients(self, x, y):
    &#34;&#34;&#34;Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\).

    Parameters
    ----------
    x : torch.Tensor
        input data `(batch, input_shape)` on compatible device with model.
    y : torch.Tensor

    Returns
    -------
    loss : torch.Tensor
    Gs : torch.Tensor
        gradients `(batch, parameters)`
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="laplace.curvature.CurvatureInterface.full"><code class="name flex">
<span>def <span class="ident">full</span></span>(<span>self, x, y, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute a dense curvature (approximation) in the form of a <span><span class="MathJax_Preview">P \times P</span><script type="math/tex">P \times P</script></span> matrix
<span><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span> with respect to parameters <span><span class="MathJax_Preview">\theta \in \mathbb{R}^P</span><script type="math/tex">\theta \in \mathbb{R}^P</script></span>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>input data <code>(batch, input_shape)</code></dd>
<dt><strong><code>y</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>labels <code>(batch, label_shape)</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>loss</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>H</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Hessian approximation <code>(parameters, parameters)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def full(self, x, y, **kwargs):
    &#34;&#34;&#34;Compute a dense curvature (approximation) in the form of a \\(P \\times P\\) matrix
    \\(H\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\).

    Parameters
    ----------
    x : torch.Tensor
        input data `(batch, input_shape)`
    y : torch.Tensor
        labels `(batch, label_shape)`

    Returns
    -------
    loss : torch.Tensor
    H : torch.Tensor
        Hessian approximation `(parameters, parameters)`
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="laplace.curvature.CurvatureInterface.kron"><code class="name flex">
<span>def <span class="ident">kron</span></span>(<span>self, x, y, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute a Kronecker factored curvature approximation (such as KFAC).
The approximation to <span><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span> takes the form of two Kronecker factors <span><span class="MathJax_Preview">Q, H</span><script type="math/tex">Q, H</script></span>,
i.e., <span><span class="MathJax_Preview">H \approx Q \otimes H</span><script type="math/tex">H \approx Q \otimes H</script></span> for each Module in the neural network permitting
such curvature.
<span><span class="MathJax_Preview">Q</span><script type="math/tex">Q</script></span> is quadratic in the input-dimension of a module <span><span class="MathJax_Preview">p_{in} \times p_{in}</span><script type="math/tex">p_{in} \times p_{in}</script></span>
and <span><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span> in the output-dimension <span><span class="MathJax_Preview">p_{out} \times p_{out}</span><script type="math/tex">p_{out} \times p_{out}</script></span>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>input data <code>(batch, input_shape)</code></dd>
<dt><strong><code>y</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>labels <code>(batch, label_shape)</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>loss</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>H</code></strong> :&ensp;<code><a title="laplace.matrix.Kron" href="../matrix.html#laplace.matrix.Kron">Kron</a></code></dt>
<dd>Kronecker factored Hessian approximation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def kron(self, x, y, **kwargs):
    &#34;&#34;&#34;Compute a Kronecker factored curvature approximation (such as KFAC).
    The approximation to \\(H\\) takes the form of two Kronecker factors \\(Q, H\\),
    i.e., \\(H \\approx Q \\otimes H\\) for each Module in the neural network permitting 
    such curvature.
    \\(Q\\) is quadratic in the input-dimension of a module \\(p_{in} \\times p_{in}\\)
    and \\(H\\) in the output-dimension \\(p_{out} \\times p_{out}\\).

    Parameters
    ----------
    x : torch.Tensor
        input data `(batch, input_shape)`
    y : torch.Tensor
        labels `(batch, label_shape)`

    Returns
    -------
    loss : torch.Tensor
    H : `laplace.matrix.Kron`
        Kronecker factored Hessian approximation.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="laplace.curvature.CurvatureInterface.diag"><code class="name flex">
<span>def <span class="ident">diag</span></span>(<span>self, x, y, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute a diagonal Hessian approximation to <span><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span> and is represented as a
vector of the dimensionality of parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>input data <code>(batch, input_shape)</code></dd>
<dt><strong><code>y</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>labels <code>(batch, label_shape)</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>loss</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>H</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>vector representing the diagonal of H</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def diag(self, x, y, **kwargs):
    &#34;&#34;&#34;Compute a diagonal Hessian approximation to \\(H\\) and is represented as a 
    vector of the dimensionality of parameters \\(\\theta\\).

    Parameters
    ----------
    x : torch.Tensor
        input data `(batch, input_shape)`
    y : torch.Tensor
        labels `(batch, label_shape)`

    Returns
    -------
    loss : torch.Tensor
    H : torch.Tensor
        vector representing the diagonal of H
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="laplace.curvature.GGNInterface"><code class="flex name class">
<span>class <span class="ident">GGNInterface</span></span>
<span>(</span><span>model, likelihood, last_layer=False, stochastic=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Generalized Gauss-Newton or Fisher Curvature Interface.
The GGN is equal to the Fisher information for the available likelihoods.
In addition to <code><a title="laplace.curvature.CurvatureInterface" href="#laplace.curvature.CurvatureInterface">CurvatureInterface</a></code>, methods for Jacobians are required by subclasses.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code> or <code><a title="laplace.feature_extractor.FeatureExtractor" href="../feature_extractor.html#laplace.feature_extractor.FeatureExtractor">FeatureExtractor</a></code></dt>
<dd>torch model (neural network)</dd>
<dt><strong><code>likelihood</code></strong> :&ensp;<code>{'classification', 'regression'}</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>last_layer</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>only consider curvature of last layer</dd>
<dt><strong><code>stochastic</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Fisher if stochastic else GGN</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GGNInterface(CurvatureInterface):
    &#34;&#34;&#34;Generalized Gauss-Newton or Fisher Curvature Interface.
    The GGN is equal to the Fisher information for the available likelihoods.
    In addition to `CurvatureInterface`, methods for Jacobians are required by subclasses.

    Parameters
    ----------
    model : torch.nn.Module or `laplace.feature_extractor.FeatureExtractor`
        torch model (neural network)
    likelihood : {&#39;classification&#39;, &#39;regression&#39;}
    last_layer : bool, default=False
        only consider curvature of last layer
    stochastic : bool, default=False
        Fisher if stochastic else GGN
    &#34;&#34;&#34;
    def __init__(self, model, likelihood, last_layer=False, stochastic=False):
        self.stochastic = stochastic
        super().__init__(model, likelihood, last_layer)

    def _get_full_ggn(self, Js, f, y):
        &#34;&#34;&#34;Compute full GGN from Jacobians.

        Parameters
        ----------
        Js : torch.Tensor
            Jacobians `(batch, parameters, outputs)`
        f : torch.Tensor
            functions `(batch, outputs)`
        y : torch.Tensor
            labels compatible with loss

        Returns
        -------
        loss : torch.Tensor
        H_ggn : torch.Tensor
            full GGN approximation `(parameters, parameters)`
        &#34;&#34;&#34;
        loss = self.factor * self.lossfunc(f, y)
        if self.likelihood == &#39;regression&#39;:
            H_ggn = torch.einsum(&#39;mkp,mkq-&gt;pq&#39;, Js, Js)
        else:
            # second derivative of log lik is diag(p) - pp^T
            ps = torch.softmax(f, dim=-1)
            H_lik = torch.diag_embed(ps) - torch.einsum(&#39;mk,mc-&gt;mck&#39;, ps, ps)
            H_ggn = torch.einsum(&#39;mcp,mck,mkq-&gt;pq&#39;, Js, H_lik, Js)
        return loss.detach(), H_ggn

    def full(self, x, y, **kwargs):
        &#34;&#34;&#34;Compute the full GGN \\(P \\times P\\) matrix as Hessian approximation
        \\(H_{ggn}\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\).
        For last-layer, reduced to \\(\\theta_{last}\\)

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)`
        y : torch.Tensor
            labels `(batch, label_shape)`

        Returns
        -------
        loss : torch.Tensor
        H_ggn : torch.Tensor
            GGN `(parameters, parameters)`
        &#34;&#34;&#34;
        if self.stochastic:
            raise ValueError(&#39;Stochastic approximation not implemented for full GGN.&#39;)

        if self.last_layer:
            Js, f = self.last_layer_jacobians(self.model, x)
        else:
            Js, f = self.jacobians(self.model, x)
        loss, H_ggn = self._get_full_ggn(Js, f, y)

        return loss, H_ggn</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="laplace.curvature.curvature.CurvatureInterface" href="curvature.html#laplace.curvature.curvature.CurvatureInterface">CurvatureInterface</a></li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="laplace.curvature.asdf.AsdfGGN" href="asdf.html#laplace.curvature.asdf.AsdfGGN">AsdfGGN</a></li>
<li><a title="laplace.curvature.backpack.BackPackGGN" href="backpack.html#laplace.curvature.backpack.BackPackGGN">BackPackGGN</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="laplace.curvature.GGNInterface.full"><code class="name flex">
<span>def <span class="ident">full</span></span>(<span>self, x, y, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the full GGN <span><span class="MathJax_Preview">P \times P</span><script type="math/tex">P \times P</script></span> matrix as Hessian approximation
<span><span class="MathJax_Preview">H_{ggn}</span><script type="math/tex">H_{ggn}</script></span> with respect to parameters <span><span class="MathJax_Preview">\theta \in \mathbb{R}^P</span><script type="math/tex">\theta \in \mathbb{R}^P</script></span>.
For last-layer, reduced to <span><span class="MathJax_Preview">\theta_{last}</span><script type="math/tex">\theta_{last}</script></span></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>input data <code>(batch, input_shape)</code></dd>
<dt><strong><code>y</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>labels <code>(batch, label_shape)</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>loss</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>H_ggn</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>GGN <code>(parameters, parameters)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def full(self, x, y, **kwargs):
    &#34;&#34;&#34;Compute the full GGN \\(P \\times P\\) matrix as Hessian approximation
    \\(H_{ggn}\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\).
    For last-layer, reduced to \\(\\theta_{last}\\)

    Parameters
    ----------
    x : torch.Tensor
        input data `(batch, input_shape)`
    y : torch.Tensor
        labels `(batch, label_shape)`

    Returns
    -------
    loss : torch.Tensor
    H_ggn : torch.Tensor
        GGN `(parameters, parameters)`
    &#34;&#34;&#34;
    if self.stochastic:
        raise ValueError(&#39;Stochastic approximation not implemented for full GGN.&#39;)

    if self.last_layer:
        Js, f = self.last_layer_jacobians(self.model, x)
    else:
        Js, f = self.jacobians(self.model, x)
    loss, H_ggn = self._get_full_ggn(Js, f, y)

    return loss, H_ggn</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="laplace.curvature.curvature.CurvatureInterface" href="curvature.html#laplace.curvature.curvature.CurvatureInterface">CurvatureInterface</a></b></code>:
<ul class="hlist">
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.diag" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.diag">diag</a></code></li>
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.gradients" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.gradients">gradients</a></code></li>
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.jacobians" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.jacobians">jacobians</a></code></li>
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.kron" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.kron">kron</a></code></li>
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.last_layer_jacobians" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.last_layer_jacobians">last_layer_jacobians</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="laplace.curvature.EFInterface"><code class="flex name class">
<span>class <span class="ident">EFInterface</span></span>
<span>(</span><span>model, likelihood, last_layer=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Interface for Empirical Fisher as Hessian approximation.
In addition to <code><a title="laplace.curvature.CurvatureInterface" href="#laplace.curvature.CurvatureInterface">CurvatureInterface</a></code>, methods for gradients are required by subclasses.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code> or <code><a title="laplace.feature_extractor.FeatureExtractor" href="../feature_extractor.html#laplace.feature_extractor.FeatureExtractor">FeatureExtractor</a></code></dt>
<dd>torch model (neural network)</dd>
<dt><strong><code>likelihood</code></strong> :&ensp;<code>{'classification', 'regression'}</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>last_layer</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>only consider curvature of last layer</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>lossfunc</code></strong> :&ensp;<code>torch.nn.MSELoss</code> or <code>torch.nn.CrossEntropyLoss</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>factor</code></strong> :&ensp;<code>float</code></dt>
<dd>conversion factor between torch losses and base likelihoods
For example, <span><span class="MathJax_Preview">\frac{1}{2}</span><script type="math/tex">\frac{1}{2}</script></span> to get to <span><span class="MathJax_Preview">\mathcal{N}(f, 1)</span><script type="math/tex">\mathcal{N}(f, 1)</script></span> from MSELoss.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EFInterface(CurvatureInterface):
    &#34;&#34;&#34;Interface for Empirical Fisher as Hessian approximation.
    In addition to `CurvatureInterface`, methods for gradients are required by subclasses.

    Parameters
    ----------
    model : torch.nn.Module or `laplace.feature_extractor.FeatureExtractor`
        torch model (neural network)
    likelihood : {&#39;classification&#39;, &#39;regression&#39;}
    last_layer : bool, default=False
        only consider curvature of last layer

    Attributes
    ----------
    lossfunc : torch.nn.MSELoss or torch.nn.CrossEntropyLoss
    factor : float
        conversion factor between torch losses and base likelihoods
        For example, \\(\\frac{1}{2}\\) to get to \\(\\mathcal{N}(f, 1)\\) from MSELoss.
    &#34;&#34;&#34;

    def full(self, x, y, **kwargs):
        &#34;&#34;&#34;Compute the full EF \\(P \\times P\\) matrix as Hessian approximation
        \\(H_{ef}\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\).
        For last-layer, reduced to \\(\\theta_{last}\\)

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)`
        y : torch.Tensor
            labels `(batch, label_shape)`

        Returns
        -------
        loss : torch.Tensor
        H_ef : torch.Tensor
            EF `(parameters, parameters)`
        &#34;&#34;&#34;
        Gs, loss = self.gradients(x, y)
        H_ef = Gs.T @ Gs
        return self.factor * loss.detach(), self.factor * H_ef</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="laplace.curvature.curvature.CurvatureInterface" href="curvature.html#laplace.curvature.curvature.CurvatureInterface">CurvatureInterface</a></li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="laplace.curvature.asdf.AsdfEF" href="asdf.html#laplace.curvature.asdf.AsdfEF">AsdfEF</a></li>
<li><a title="laplace.curvature.backpack.BackPackEF" href="backpack.html#laplace.curvature.backpack.BackPackEF">BackPackEF</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="laplace.curvature.EFInterface.full"><code class="name flex">
<span>def <span class="ident">full</span></span>(<span>self, x, y, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the full EF <span><span class="MathJax_Preview">P \times P</span><script type="math/tex">P \times P</script></span> matrix as Hessian approximation
<span><span class="MathJax_Preview">H_{ef}</span><script type="math/tex">H_{ef}</script></span> with respect to parameters <span><span class="MathJax_Preview">\theta \in \mathbb{R}^P</span><script type="math/tex">\theta \in \mathbb{R}^P</script></span>.
For last-layer, reduced to <span><span class="MathJax_Preview">\theta_{last}</span><script type="math/tex">\theta_{last}</script></span></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>input data <code>(batch, input_shape)</code></dd>
<dt><strong><code>y</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>labels <code>(batch, label_shape)</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>loss</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>H_ef</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>EF <code>(parameters, parameters)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def full(self, x, y, **kwargs):
    &#34;&#34;&#34;Compute the full EF \\(P \\times P\\) matrix as Hessian approximation
    \\(H_{ef}\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\).
    For last-layer, reduced to \\(\\theta_{last}\\)

    Parameters
    ----------
    x : torch.Tensor
        input data `(batch, input_shape)`
    y : torch.Tensor
        labels `(batch, label_shape)`

    Returns
    -------
    loss : torch.Tensor
    H_ef : torch.Tensor
        EF `(parameters, parameters)`
    &#34;&#34;&#34;
    Gs, loss = self.gradients(x, y)
    H_ef = Gs.T @ Gs
    return self.factor * loss.detach(), self.factor * H_ef</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="laplace.curvature.curvature.CurvatureInterface" href="curvature.html#laplace.curvature.curvature.CurvatureInterface">CurvatureInterface</a></b></code>:
<ul class="hlist">
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.diag" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.diag">diag</a></code></li>
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.gradients" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.gradients">gradients</a></code></li>
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.jacobians" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.jacobians">jacobians</a></code></li>
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.kron" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.kron">kron</a></code></li>
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.last_layer_jacobians" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.last_layer_jacobians">last_layer_jacobians</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="laplace.curvature.BackPackInterface"><code class="flex name class">
<span>class <span class="ident">BackPackInterface</span></span>
<span>(</span><span>model, likelihood, last_layer=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Interface for Backpack backend.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BackPackInterface(CurvatureInterface):
    &#34;&#34;&#34;Interface for Backpack backend.
    &#34;&#34;&#34;
    def __init__(self, model, likelihood, last_layer=False):
        super().__init__(model, likelihood, last_layer)
        extend(self._model)
        extend(self.lossfunc)

    @staticmethod
    def jacobians(model, x):
        &#34;&#34;&#34;Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\)
        using backpack&#39;s BatchGrad per output dimension.

        Parameters
        ----------
        model : torch.nn.Module
        x : torch.Tensor
            input data `(batch, input_shape)` on compatible device with model.

        Returns
        -------
        Js : torch.Tensor
            Jacobians `(batch, parameters, outputs)`
        f : torch.Tensor
            output function `(batch, outputs)`
        &#34;&#34;&#34;
        model = extend(model)
        to_stack = []
        for i in range(model.output_size):
            model.zero_grad()
            out = model(x)
            with backpack(BatchGrad()):
                if model.output_size &gt; 1:
                    out[:, i].sum().backward()
                else:
                    out.sum().backward()
                to_cat = []
                for param in model.parameters():
                    to_cat.append(param.grad_batch.detach().reshape(x.shape[0], -1))
                    delattr(param, &#39;grad_batch&#39;)
                Jk = torch.cat(to_cat, dim=1)
            to_stack.append(Jk)
            if i == 0:
                f = out.detach()

        model.zero_grad()
        CTX.remove_hooks()
        _cleanup(model)
        if model.output_size &gt; 1:
            return torch.stack(to_stack, dim=2).transpose(1, 2), f
        else:
            return Jk.unsqueeze(-1).transpose(1, 2), f

    def gradients(self, x, y):
        &#34;&#34;&#34;Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter
        \\(\\theta\\) using Backpack&#39;s BatchGrad.

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)` on compatible device with model.
        y : torch.Tensor

        Returns
        -------
        loss : torch.Tensor
        Gs : torch.Tensor
            gradients `(batch, parameters)`
        &#34;&#34;&#34;
        f = self.model(x)
        loss = self.lossfunc(f, y)
        with backpack(BatchGrad()):
            loss.backward()
        Gs = torch.cat([p.grad_batch.data.flatten(start_dim=1)
                        for p in self._model.parameters()], dim=1)
        return Gs, loss</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="laplace.curvature.curvature.CurvatureInterface" href="curvature.html#laplace.curvature.curvature.CurvatureInterface">CurvatureInterface</a></li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="laplace.curvature.backpack.BackPackEF" href="backpack.html#laplace.curvature.backpack.BackPackEF">BackPackEF</a></li>
<li><a title="laplace.curvature.backpack.BackPackGGN" href="backpack.html#laplace.curvature.backpack.BackPackGGN">BackPackGGN</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="laplace.curvature.BackPackInterface.jacobians"><code class="name flex">
<span>def <span class="ident">jacobians</span></span>(<span>model, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute Jacobians <span><span class="MathJax_Preview">\nabla_{\theta} f(x;\theta)</span><script type="math/tex">\nabla_{\theta} f(x;\theta)</script></span> at current parameter <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>
using backpack's BatchGrad per output dimension.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>input data <code>(batch, input_shape)</code> on compatible device with model.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Js</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Jacobians <code>(batch, parameters, outputs)</code></dd>
<dt><strong><code>f</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>output function <code>(batch, outputs)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def jacobians(model, x):
    &#34;&#34;&#34;Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\)
    using backpack&#39;s BatchGrad per output dimension.

    Parameters
    ----------
    model : torch.nn.Module
    x : torch.Tensor
        input data `(batch, input_shape)` on compatible device with model.

    Returns
    -------
    Js : torch.Tensor
        Jacobians `(batch, parameters, outputs)`
    f : torch.Tensor
        output function `(batch, outputs)`
    &#34;&#34;&#34;
    model = extend(model)
    to_stack = []
    for i in range(model.output_size):
        model.zero_grad()
        out = model(x)
        with backpack(BatchGrad()):
            if model.output_size &gt; 1:
                out[:, i].sum().backward()
            else:
                out.sum().backward()
            to_cat = []
            for param in model.parameters():
                to_cat.append(param.grad_batch.detach().reshape(x.shape[0], -1))
                delattr(param, &#39;grad_batch&#39;)
            Jk = torch.cat(to_cat, dim=1)
        to_stack.append(Jk)
        if i == 0:
            f = out.detach()

    model.zero_grad()
    CTX.remove_hooks()
    _cleanup(model)
    if model.output_size &gt; 1:
        return torch.stack(to_stack, dim=2).transpose(1, 2), f
    else:
        return Jk.unsqueeze(-1).transpose(1, 2), f</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="laplace.curvature.BackPackInterface.gradients"><code class="name flex">
<span>def <span class="ident">gradients</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute gradients <span><span class="MathJax_Preview">\nabla_\theta \ell(f(x;\theta, y)</span><script type="math/tex">\nabla_\theta \ell(f(x;\theta, y)</script></span> at current parameter
<span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> using Backpack's BatchGrad.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>input data <code>(batch, input_shape)</code> on compatible device with model.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>loss</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>Gs</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>gradients <code>(batch, parameters)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gradients(self, x, y):
    &#34;&#34;&#34;Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter
    \\(\\theta\\) using Backpack&#39;s BatchGrad.

    Parameters
    ----------
    x : torch.Tensor
        input data `(batch, input_shape)` on compatible device with model.
    y : torch.Tensor

    Returns
    -------
    loss : torch.Tensor
    Gs : torch.Tensor
        gradients `(batch, parameters)`
    &#34;&#34;&#34;
    f = self.model(x)
    loss = self.lossfunc(f, y)
    with backpack(BatchGrad()):
        loss.backward()
    Gs = torch.cat([p.grad_batch.data.flatten(start_dim=1)
                    for p in self._model.parameters()], dim=1)
    return Gs, loss</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="laplace.curvature.curvature.CurvatureInterface" href="curvature.html#laplace.curvature.curvature.CurvatureInterface">CurvatureInterface</a></b></code>:
<ul class="hlist">
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.diag" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.diag">diag</a></code></li>
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.full" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.full">full</a></code></li>
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.kron" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.kron">kron</a></code></li>
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.last_layer_jacobians" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.last_layer_jacobians">last_layer_jacobians</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="laplace.curvature.BackPackGGN"><code class="flex name class">
<span>class <span class="ident">BackPackGGN</span></span>
<span>(</span><span>model, likelihood, last_layer=False, stochastic=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Implementation of the <code><a title="laplace.curvature.GGNInterface" href="#laplace.curvature.GGNInterface">GGNInterface</a></code> using Backpack.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BackPackGGN(BackPackInterface, GGNInterface):
    &#34;&#34;&#34;Implementation of the `GGNInterface` using Backpack.
    &#34;&#34;&#34;
    def __init__(self, model, likelihood, last_layer=False, stochastic=False):
        super().__init__(model, likelihood, last_layer)
        self.stochastic = stochastic

    def _get_diag_ggn(self):
        if self.stochastic:
            return torch.cat([p.diag_ggn_mc.data.flatten() for p in self._model.parameters()])
        else:
            return torch.cat([p.diag_ggn_exact.data.flatten() for p in self._model.parameters()])

    def _get_kron_factors(self):
        if self.stochastic:
            return Kron([p.kfac for p in self._model.parameters()])
        else:
            return Kron([p.kflr for p in self._model.parameters()])

    @staticmethod
    def _rescale_kron_factors(kron, M, N):
        # Renormalize Kronecker factor to sum up correctly over N data points with batches of M
        # for M=N (full-batch) just M/N=1
        for F in kron.kfacs:
            if len(F) == 2:
                F[1] *= M/N
        return kron

    def diag(self, X, y, **kwargs):
        context = DiagGGNMC if self.stochastic else DiagGGNExact
        f = self.model(X)
        loss = self.lossfunc(f, y)
        with backpack(context()):
            loss.backward()
        dggn = self._get_diag_ggn()

        return self.factor * loss.detach(), self.factor * dggn

    def kron(self, X, y, N, **kwargs) -&gt; [torch.Tensor, Kron]:
        context = KFAC if self.stochastic else KFLR
        f = self.model(X)
        loss = self.lossfunc(f, y)
        with backpack(context()):
            loss.backward()
        kron = self._get_kron_factors()
        kron = self._rescale_kron_factors(kron, len(y), N)

        return self.factor * loss.detach(), self.factor * kron</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="laplace.curvature.backpack.BackPackInterface" href="backpack.html#laplace.curvature.backpack.BackPackInterface">BackPackInterface</a></li>
<li><a title="laplace.curvature.curvature.GGNInterface" href="curvature.html#laplace.curvature.curvature.GGNInterface">GGNInterface</a></li>
<li><a title="laplace.curvature.curvature.CurvatureInterface" href="curvature.html#laplace.curvature.curvature.CurvatureInterface">CurvatureInterface</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="laplace.curvature.backpack.BackPackInterface" href="backpack.html#laplace.curvature.backpack.BackPackInterface">BackPackInterface</a></b></code>:
<ul class="hlist">
<li><code><a title="laplace.curvature.backpack.BackPackInterface.diag" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.diag">diag</a></code></li>
<li><code><a title="laplace.curvature.backpack.BackPackInterface.full" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.full">full</a></code></li>
<li><code><a title="laplace.curvature.backpack.BackPackInterface.gradients" href="backpack.html#laplace.curvature.backpack.BackPackInterface.gradients">gradients</a></code></li>
<li><code><a title="laplace.curvature.backpack.BackPackInterface.jacobians" href="backpack.html#laplace.curvature.backpack.BackPackInterface.jacobians">jacobians</a></code></li>
<li><code><a title="laplace.curvature.backpack.BackPackInterface.kron" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.kron">kron</a></code></li>
<li><code><a title="laplace.curvature.backpack.BackPackInterface.last_layer_jacobians" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.last_layer_jacobians">last_layer_jacobians</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="laplace.curvature.BackPackEF"><code class="flex name class">
<span>class <span class="ident">BackPackEF</span></span>
<span>(</span><span>model, likelihood, last_layer=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Implementation of <code><a title="laplace.curvature.EFInterface" href="#laplace.curvature.EFInterface">EFInterface</a></code> using Backpack.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BackPackEF(BackPackInterface, EFInterface):
    &#34;&#34;&#34;Implementation of `EFInterface` using Backpack.
    &#34;&#34;&#34;

    def diag(self, X, y, **kwargs):
        f = self.model(X)
        loss = self.lossfunc(f, y)
        with backpack(SumGradSquared()):
            loss.backward()
        diag_EF = torch.cat([p.sum_grad_squared.data.flatten()
                             for p in self._model.parameters()])

        return self.factor * loss.detach(), self.factor * diag_EF

    def kron(self, X, y, **kwargs):
        raise NotImplementedError(&#39;Unavailable through Backpack.&#39;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="laplace.curvature.backpack.BackPackInterface" href="backpack.html#laplace.curvature.backpack.BackPackInterface">BackPackInterface</a></li>
<li><a title="laplace.curvature.curvature.EFInterface" href="curvature.html#laplace.curvature.curvature.EFInterface">EFInterface</a></li>
<li><a title="laplace.curvature.curvature.CurvatureInterface" href="curvature.html#laplace.curvature.curvature.CurvatureInterface">CurvatureInterface</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="laplace.curvature.backpack.BackPackInterface" href="backpack.html#laplace.curvature.backpack.BackPackInterface">BackPackInterface</a></b></code>:
<ul class="hlist">
<li><code><a title="laplace.curvature.backpack.BackPackInterface.diag" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.diag">diag</a></code></li>
<li><code><a title="laplace.curvature.backpack.BackPackInterface.full" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.full">full</a></code></li>
<li><code><a title="laplace.curvature.backpack.BackPackInterface.gradients" href="backpack.html#laplace.curvature.backpack.BackPackInterface.gradients">gradients</a></code></li>
<li><code><a title="laplace.curvature.backpack.BackPackInterface.jacobians" href="backpack.html#laplace.curvature.backpack.BackPackInterface.jacobians">jacobians</a></code></li>
<li><code><a title="laplace.curvature.backpack.BackPackInterface.kron" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.kron">kron</a></code></li>
<li><code><a title="laplace.curvature.backpack.BackPackInterface.last_layer_jacobians" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.last_layer_jacobians">last_layer_jacobians</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="laplace.curvature.AsdfInterface"><code class="flex name class">
<span>class <span class="ident">AsdfInterface</span></span>
<span>(</span><span>model, likelihood, last_layer=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Interface for asdfghjkl backend.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AsdfInterface(CurvatureInterface):
    &#34;&#34;&#34;Interface for asdfghjkl backend.
    &#34;&#34;&#34;
    def __init__(self, model, likelihood, last_layer=False):
        if likelihood != &#39;classification&#39;:
            raise ValueError(&#39;This backend only supports classification currently.&#39;)
        super().__init__(model, likelihood, last_layer)

    @staticmethod
    def jacobians(model, x):
        &#34;&#34;&#34;Compute Jacobians \\(\\nabla_\\theta f(x;\\theta)\\) at current parameter \\(\\theta\\)
        using asdfghjkl&#39;s gradient per output dimension.

        Parameters
        ----------
        model : torch.nn.Module
        x : torch.Tensor
            input data `(batch, input_shape)` on compatible device with model.

        Returns
        -------
        Js : torch.Tensor
            Jacobians `(batch, parameters, outputs)`
        f : torch.Tensor
            output function `(batch, outputs)`
        &#34;&#34;&#34;
        Js = list()
        for i in range(model.output_size):
            def loss_fn(outputs, targets):
                return outputs[:, i].sum()

            f = batch_gradient(model, loss_fn, x, None).detach()
            Js.append(_get_batch_grad(model))
        Js = torch.stack(Js, dim=1)
        return Js, f

    def gradients(self, x, y):
        &#34;&#34;&#34;Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter 
        \\(\\theta\\) using asdfghjkl&#39;s backend.

        Parameters
        ----------
        x : torch.Tensor
            input data `(batch, input_shape)` on compatible device with model.
        y : torch.Tensor

        Returns
        -------
        loss : torch.Tensor
        Gs : torch.Tensor
            gradients `(batch, parameters)`
        &#34;&#34;&#34;
        f = batch_gradient(self.model, self.lossfunc, x, y).detach()
        Gs = _get_batch_grad(self._model)
        loss = self.lossfunc(f, y)
        return Gs, loss

    @abstractproperty
    def _ggn_type(self):
        raise NotImplementedError()

    def _get_kron_factors(self, curv, M):
        kfacs = list()
        for module in curv._model.modules():
            if _is_batchnorm(module):
                warnings.warn(&#39;BatchNorm unsupported for Kron, ignore.&#39;)
                continue

            stats = getattr(module, self._ggn_type, None)
            if stats is None:
                continue
            if hasattr(module, &#39;bias&#39;) and module.bias is not None:
                # split up bias and weights
                kfacs.append([stats.kron.B, stats.kron.A[:-1, :-1]])
                kfacs.append([stats.kron.B * stats.kron.A[-1, -1] / M])
            elif hasattr(module, &#39;weight&#39;):
                p, q = np.prod(stats.kron.B.shape), np.prod(stats.kron.A.shape)
                if p == q == 1:
                    kfacs.append([stats.kron.B * stats.kron.A])
                else:
                    kfacs.append([stats.kron.B, stats.kron.A])
            else:
                raise ValueError(f&#39;Whats happening with {module}?&#39;)
        return Kron(kfacs)

    @staticmethod
    def _rescale_kron_factors(kron, N):
        for F in kron.kfacs:
            if len(F) == 2:
                F[1] *= 1/N
        return kron

    def diag(self, X, y, **kwargs):
        with torch.no_grad():
            if self.last_layer:
                f, X = self.model.forward_with_features(X)
            else:
                f = self.model(X)
            loss = self.lossfunc(f, y)
        curv = fisher_for_cross_entropy(self._model, self._ggn_type, SHAPE_DIAG,
                                        inputs=X, targets=y)
        diag_ggn = curv.matrices_to_vector(None)
        return self.factor * loss, self.factor * diag_ggn

    def kron(self, X, y, N, **wkwargs) -&gt; [torch.Tensor, Kron]:
        with torch.no_grad():
            if self.last_layer:
                f, X = self.model.forward_with_features(X)
            else:
                f = self.model(X)
            loss = self.lossfunc(f, y)
        curv = fisher_for_cross_entropy(self._model, self._ggn_type, SHAPE_KRON,
                                        inputs=X, targets=y)
        M = len(y)
        kron = self._get_kron_factors(curv, M)
        kron = self._rescale_kron_factors(kron, N)
        return self.factor * loss, self.factor * kron</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="laplace.curvature.curvature.CurvatureInterface" href="curvature.html#laplace.curvature.curvature.CurvatureInterface">CurvatureInterface</a></li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="laplace.curvature.asdf.AsdfEF" href="asdf.html#laplace.curvature.asdf.AsdfEF">AsdfEF</a></li>
<li><a title="laplace.curvature.asdf.AsdfGGN" href="asdf.html#laplace.curvature.asdf.AsdfGGN">AsdfGGN</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="laplace.curvature.AsdfInterface.jacobians"><code class="name flex">
<span>def <span class="ident">jacobians</span></span>(<span>model, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute Jacobians <span><span class="MathJax_Preview">\nabla_\theta f(x;\theta)</span><script type="math/tex">\nabla_\theta f(x;\theta)</script></span> at current parameter <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>
using asdfghjkl's gradient per output dimension.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>input data <code>(batch, input_shape)</code> on compatible device with model.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Js</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Jacobians <code>(batch, parameters, outputs)</code></dd>
<dt><strong><code>f</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>output function <code>(batch, outputs)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def jacobians(model, x):
    &#34;&#34;&#34;Compute Jacobians \\(\\nabla_\\theta f(x;\\theta)\\) at current parameter \\(\\theta\\)
    using asdfghjkl&#39;s gradient per output dimension.

    Parameters
    ----------
    model : torch.nn.Module
    x : torch.Tensor
        input data `(batch, input_shape)` on compatible device with model.

    Returns
    -------
    Js : torch.Tensor
        Jacobians `(batch, parameters, outputs)`
    f : torch.Tensor
        output function `(batch, outputs)`
    &#34;&#34;&#34;
    Js = list()
    for i in range(model.output_size):
        def loss_fn(outputs, targets):
            return outputs[:, i].sum()

        f = batch_gradient(model, loss_fn, x, None).detach()
        Js.append(_get_batch_grad(model))
    Js = torch.stack(Js, dim=1)
    return Js, f</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="laplace.curvature.AsdfInterface.gradients"><code class="name flex">
<span>def <span class="ident">gradients</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute gradients <span><span class="MathJax_Preview">\nabla_\theta \ell(f(x;\theta, y)</span><script type="math/tex">\nabla_\theta \ell(f(x;\theta, y)</script></span> at current parameter
<span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> using asdfghjkl's backend.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>input data <code>(batch, input_shape)</code> on compatible device with model.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>loss</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>Gs</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>gradients <code>(batch, parameters)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gradients(self, x, y):
    &#34;&#34;&#34;Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter 
    \\(\\theta\\) using asdfghjkl&#39;s backend.

    Parameters
    ----------
    x : torch.Tensor
        input data `(batch, input_shape)` on compatible device with model.
    y : torch.Tensor

    Returns
    -------
    loss : torch.Tensor
    Gs : torch.Tensor
        gradients `(batch, parameters)`
    &#34;&#34;&#34;
    f = batch_gradient(self.model, self.lossfunc, x, y).detach()
    Gs = _get_batch_grad(self._model)
    loss = self.lossfunc(f, y)
    return Gs, loss</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="laplace.curvature.curvature.CurvatureInterface" href="curvature.html#laplace.curvature.curvature.CurvatureInterface">CurvatureInterface</a></b></code>:
<ul class="hlist">
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.diag" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.diag">diag</a></code></li>
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.full" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.full">full</a></code></li>
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.kron" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.kron">kron</a></code></li>
<li><code><a title="laplace.curvature.curvature.CurvatureInterface.last_layer_jacobians" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.last_layer_jacobians">last_layer_jacobians</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="laplace.curvature.AsdfGGN"><code class="flex name class">
<span>class <span class="ident">AsdfGGN</span></span>
<span>(</span><span>model, likelihood, last_layer=False, stochastic=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Implementation of the <code><a title="laplace.curvature.GGNInterface" href="#laplace.curvature.GGNInterface">GGNInterface</a></code> using asdfghjkl.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AsdfGGN(AsdfInterface, GGNInterface):
    &#34;&#34;&#34;Implementation of the `GGNInterface` using asdfghjkl.
    &#34;&#34;&#34;
    def __init__(self, model, likelihood, last_layer=False, stochastic=False):
        super().__init__(model, likelihood, last_layer)
        self.stochastic = stochastic

    @property
    def _ggn_type(self):
        return FISHER_MC if self.stochastic else FISHER_EXACT</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="laplace.curvature.asdf.AsdfInterface" href="asdf.html#laplace.curvature.asdf.AsdfInterface">AsdfInterface</a></li>
<li><a title="laplace.curvature.curvature.GGNInterface" href="curvature.html#laplace.curvature.curvature.GGNInterface">GGNInterface</a></li>
<li><a title="laplace.curvature.curvature.CurvatureInterface" href="curvature.html#laplace.curvature.curvature.CurvatureInterface">CurvatureInterface</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="laplace.curvature.asdf.AsdfInterface" href="asdf.html#laplace.curvature.asdf.AsdfInterface">AsdfInterface</a></b></code>:
<ul class="hlist">
<li><code><a title="laplace.curvature.asdf.AsdfInterface.diag" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.diag">diag</a></code></li>
<li><code><a title="laplace.curvature.asdf.AsdfInterface.full" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.full">full</a></code></li>
<li><code><a title="laplace.curvature.asdf.AsdfInterface.gradients" href="asdf.html#laplace.curvature.asdf.AsdfInterface.gradients">gradients</a></code></li>
<li><code><a title="laplace.curvature.asdf.AsdfInterface.jacobians" href="asdf.html#laplace.curvature.asdf.AsdfInterface.jacobians">jacobians</a></code></li>
<li><code><a title="laplace.curvature.asdf.AsdfInterface.kron" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.kron">kron</a></code></li>
<li><code><a title="laplace.curvature.asdf.AsdfInterface.last_layer_jacobians" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.last_layer_jacobians">last_layer_jacobians</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="laplace.curvature.AsdfEF"><code class="flex name class">
<span>class <span class="ident">AsdfEF</span></span>
<span>(</span><span>model, likelihood, last_layer=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Implementation of the <code><a title="laplace.curvature.EFInterface" href="#laplace.curvature.EFInterface">EFInterface</a></code> using asdfghjkl.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AsdfEF(AsdfInterface, EFInterface):
    &#34;&#34;&#34;Implementation of the `EFInterface` using asdfghjkl.
    &#34;&#34;&#34;
    
    @property
    def _ggn_type(self):
        return COV</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="laplace.curvature.asdf.AsdfInterface" href="asdf.html#laplace.curvature.asdf.AsdfInterface">AsdfInterface</a></li>
<li><a title="laplace.curvature.curvature.EFInterface" href="curvature.html#laplace.curvature.curvature.EFInterface">EFInterface</a></li>
<li><a title="laplace.curvature.curvature.CurvatureInterface" href="curvature.html#laplace.curvature.curvature.CurvatureInterface">CurvatureInterface</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="laplace.curvature.asdf.AsdfInterface" href="asdf.html#laplace.curvature.asdf.AsdfInterface">AsdfInterface</a></b></code>:
<ul class="hlist">
<li><code><a title="laplace.curvature.asdf.AsdfInterface.diag" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.diag">diag</a></code></li>
<li><code><a title="laplace.curvature.asdf.AsdfInterface.full" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.full">full</a></code></li>
<li><code><a title="laplace.curvature.asdf.AsdfInterface.gradients" href="asdf.html#laplace.curvature.asdf.AsdfInterface.gradients">gradients</a></code></li>
<li><code><a title="laplace.curvature.asdf.AsdfInterface.jacobians" href="asdf.html#laplace.curvature.asdf.AsdfInterface.jacobians">jacobians</a></code></li>
<li><code><a title="laplace.curvature.asdf.AsdfInterface.kron" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.kron">kron</a></code></li>
<li><code><a title="laplace.curvature.asdf.AsdfInterface.last_layer_jacobians" href="curvature.html#laplace.curvature.curvature.CurvatureInterface.last_layer_jacobians">last_layer_jacobians</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="laplace" href="../index.html">laplace</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="laplace.curvature.asdf" href="asdf.html">laplace.curvature.asdf</a></code></li>
<li><code><a title="laplace.curvature.backpack" href="backpack.html">laplace.curvature.backpack</a></code></li>
<li><code><a title="laplace.curvature.curvature" href="curvature.html">laplace.curvature.curvature</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="laplace.curvature.CurvatureInterface" href="#laplace.curvature.CurvatureInterface">CurvatureInterface</a></code></h4>
<ul class="">
<li><code><a title="laplace.curvature.CurvatureInterface.jacobians" href="#laplace.curvature.CurvatureInterface.jacobians">jacobians</a></code></li>
<li><code><a title="laplace.curvature.CurvatureInterface.last_layer_jacobians" href="#laplace.curvature.CurvatureInterface.last_layer_jacobians">last_layer_jacobians</a></code></li>
<li><code><a title="laplace.curvature.CurvatureInterface.gradients" href="#laplace.curvature.CurvatureInterface.gradients">gradients</a></code></li>
<li><code><a title="laplace.curvature.CurvatureInterface.full" href="#laplace.curvature.CurvatureInterface.full">full</a></code></li>
<li><code><a title="laplace.curvature.CurvatureInterface.kron" href="#laplace.curvature.CurvatureInterface.kron">kron</a></code></li>
<li><code><a title="laplace.curvature.CurvatureInterface.diag" href="#laplace.curvature.CurvatureInterface.diag">diag</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laplace.curvature.GGNInterface" href="#laplace.curvature.GGNInterface">GGNInterface</a></code></h4>
<ul class="">
<li><code><a title="laplace.curvature.GGNInterface.full" href="#laplace.curvature.GGNInterface.full">full</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laplace.curvature.EFInterface" href="#laplace.curvature.EFInterface">EFInterface</a></code></h4>
<ul class="">
<li><code><a title="laplace.curvature.EFInterface.full" href="#laplace.curvature.EFInterface.full">full</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laplace.curvature.BackPackInterface" href="#laplace.curvature.BackPackInterface">BackPackInterface</a></code></h4>
<ul class="">
<li><code><a title="laplace.curvature.BackPackInterface.jacobians" href="#laplace.curvature.BackPackInterface.jacobians">jacobians</a></code></li>
<li><code><a title="laplace.curvature.BackPackInterface.gradients" href="#laplace.curvature.BackPackInterface.gradients">gradients</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laplace.curvature.BackPackGGN" href="#laplace.curvature.BackPackGGN">BackPackGGN</a></code></h4>
</li>
<li>
<h4><code><a title="laplace.curvature.BackPackEF" href="#laplace.curvature.BackPackEF">BackPackEF</a></code></h4>
</li>
<li>
<h4><code><a title="laplace.curvature.AsdfInterface" href="#laplace.curvature.AsdfInterface">AsdfInterface</a></code></h4>
<ul class="">
<li><code><a title="laplace.curvature.AsdfInterface.jacobians" href="#laplace.curvature.AsdfInterface.jacobians">jacobians</a></code></li>
<li><code><a title="laplace.curvature.AsdfInterface.gradients" href="#laplace.curvature.AsdfInterface.gradients">gradients</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laplace.curvature.AsdfGGN" href="#laplace.curvature.AsdfGGN">AsdfGGN</a></code></h4>
</li>
<li>
<h4><code><a title="laplace.curvature.AsdfEF" href="#laplace.curvature.AsdfEF">AsdfEF</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>